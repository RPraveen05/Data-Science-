{\rtf1\ansi\ansicpg1252\deff0\nouicompat\deflang1033{\fonttbl{\f0\fnil\fcharset0 Calibri;}{\f1\fnil\fcharset0 Times New Roman;}}
{\*\generator Riched20 10.0.22621}\viewkind4\uc1 
\pard\sa200\sl276\slmult1\qc\f0\fs48\lang9                                                                                       \par
\b\f1 SMS Spam Detection  \b0\f0\par

\pard\sa200\sl276\slmult1\ul\b\fs32 Program\par
\ulnone\b0\fs22 import streamlit as st\par
import pickle\par
import string\par
from nltk.corpus import stopwords\par
import nltk\par
from nltk.stem.porter import PorterStemmer\par
\par
ps = PorterStemmer()\par
\par
\par
def transform_text(text):\par
    text = text.lower()\par
    text = nltk.word_tokenize(text)\par
\par
    y = []\par
    for i in text:\par
        if i.isalnum():\par
            y.append(i)\par
\par
    text = y[:]\par
    y.clear()\par
\par
    for i in text:\par
        if i not in stopwords.words('english') and i not in string.punctuation:\par
            y.append(i)\par
\par
    text = y[:]\par
    y.clear()\par
\par
    for i in text:\par
        y.append(ps.stem(i))\par
\par
    return " ".join(y)\par
\par
tfidf = pickle.load(open('vectorizer.pkl','rb'))\par
model = pickle.load(open('model.pkl','rb'))\par
\par
st.title("Email/SMS Spam Classifier")\par
\par
input_sms = st.text_area("Enter the message")\par
\par
if st.button('Predict'):\par
\par
    # 1. preprocess\par
    transformed_sms = transform_text(input_sms)\par
    # 2. vectorize\par
    vector_input = tfidf.transform([transformed_sms])\par
    # 3. predict\par
    result = model.predict(vector_input)[0]\par
    # 4. Display\par
    if result == 1:\par
        st.header("Spam")\par
    else:\par
        st.header("Not Spam")import streamlit as st\par
import pickle\par
import string\par
from nltk.corpus import stopwords\par
import nltk\par
from nltk.stem.porter import PorterStemmer\par
\par
ps = PorterStemmer()\par
\par
\par
def transform_text(text):\par
    text = text.lower()\par
    text = nltk.word_tokenize(text)\par
\par
    y = []\par
    for i in text:\par
        if i.isalnum():\par
            y.append(i)\par
\par
    text = y[:]\par
    y.clear()\par
\par
    for i in text:\par
        if i not in stopwords.words('english') and i not in string.punctuation:\par
            y.append(i)\par
\par
    text = y[:]\par
    y.clear()\par
\par
    for i in text:\par
        y.append(ps.stem(i))\par
\par
    return " ".join(y)\par
\par
tfidf = pickle.load(open('vectorizer.pkl','rb'))\par
model = pickle.load(open('model.pkl','rb'))\par
\par
st.title("Email/SMS Spam Classifier")\par
\par
input_sms = st.text_area("Enter the message")\par
\par
if st.button('Predict'):\par
\par
    # 1. preprocess\par
    transformed_sms = transform_text(input_sms)\par
    # 2. vectorize\par
    vector_input = tfidf.transform([transformed_sms])\par
    # 3. predict\par
    result = model.predict(vector_input)[0]\par
    # 4. Display\par
    if result == 1:\par
        st.header("Spam")\par
    else:\par
        st.header("Not Spam")\par
import numpy as np\par
import pandas as pd\par
df = pd.read_csv('spam.csv')\par
df.sample(5)\par
v1\tab v2\tab Unnamed: 2\tab Unnamed: 3\tab Unnamed: 4\par
2464\tab ham\tab They will pick up and drop in car.so no problem..\tab NaN\tab NaN\tab NaN\par
1248\tab ham\tab HI HUN! IM NOT COMIN 2NITE-TELL EVERY1 IM SORR...\tab NaN\tab NaN\tab NaN\par
1413\tab spam\tab Dear U've been invited to XCHAT. This is our f...\tab NaN\tab NaN\tab NaN\par
2995\tab ham\tab They released vday shirts and when u put it on...\tab NaN\tab NaN\tab NaN\par
4458\tab spam\tab Welcome to UK-mobile-date this msg is FREE giv...\tab NaN\tab NaN\tab NaN\par
df.shape\par
(5572, 5)\par
# 1. Data cleaning\par
# 2. EDA\par
# 3. Text Preprocessing\par
# 4. Model building\par
# 5. Evaluation\par
# 6. Improvement\par
# 7. Website\par
# 8. Deploy\par
1. Data Cleaning\par
df.info()\par
<class 'pandas.core.frame.DataFrame'>\par
RangeIndex: 5572 entries, 0 to 5571\par
Data columns (total 5 columns):\par
 #   Column      Non-Null Count  Dtype \par
---  ------      --------------  ----- \par
 0   v1          5572 non-null   object\par
 1   v2          5572 non-null   object\par
 2   Unnamed: 2  50 non-null     object\par
 3   Unnamed: 3  12 non-null     object\par
 4   Unnamed: 4  6 non-null      object\par
dtypes: object(5)\par
memory usage: 217.8+ KB\par
# drop last 3 cols\par
df.drop(columns=['Unnamed: 2','Unnamed: 3','Unnamed: 4'],inplace=True)\par
df.sample(5)\par
v1\tab v2\par
1947\tab ham\tab The battery is for mr adewale my uncle. Aka Egbon\par
2712\tab ham\tab Hey you still want to go for yogasana? Coz if ...\par
4428\tab ham\tab Hey they r not watching movie tonight so i'll ...\par
3944\tab ham\tab I will be gentle princess! We will make sweet ...\par
49\tab ham\tab U don't know how stubborn I am. I didn't even ...\par
# renaming the cols\par
df.rename(columns=\{'v1':'target','v2':'text'\},inplace=True)\par
df.sample(5)\par
target\tab text\par
1418\tab ham\tab Lmao. Take a pic and send it to me.\par
2338\tab ham\tab Alright, see you in a bit\par
88\tab ham\tab I'm really not up to it still tonight babe\par
3735\tab ham\tab Hows the street where the end of library walk is?\par
3859\tab ham\tab Yep. I do like the pink furniture tho.\par
from sklearn.preprocessing import LabelEncoder\par
encoder = LabelEncoder()\par
df['target'] = encoder.fit_transform(df['target'])\par
df.head()\par
target\tab text\par
0\tab 0\tab Go until jurong point, crazy.. Available only ...\par
1\tab 0\tab Ok lar... Joking wif u oni...\par
2\tab 1\tab Free entry in 2 a wkly comp to win FA Cup fina...\par
3\tab 0\tab U dun say so early hor... U c already then say...\par
4\tab 0\tab Nah I don't think he goes to usf, he lives aro...\par
# missing values\par
df.isnull().sum()\par
target    0\par
text      0\par
dtype: int64\par
# check for duplicate values\par
df.duplicated().sum()\par
403\par
# remove duplicates\par
df = df.drop_duplicates(keep='first')\par
df.duplicated().sum()\par
0\par
df.shape\par
(5169, 2)\par
2.EDA\par
df.head()\par
target\tab text\par
0\tab 0\tab Go until jurong point, crazy.. Available only ...\par
1\tab 0\tab Ok lar... Joking wif u oni...\par
2\tab 1\tab Free entry in 2 a wkly comp to win FA Cup fina...\par
3\tab 0\tab U dun say so early hor... U c already then say...\par
4\tab 0\tab Nah I don't think he goes to usf, he lives aro...\par
df['target'].value_counts()\par
0    4516\par
1     653\par
Name: target, dtype: int64\par
import matplotlib.pyplot as plt\par
plt.pie(df['target'].value_counts(), labels=['ham','spam'],autopct="%0.2f")\par
plt.show()\par
\par
# Data is imbalanced\par
import nltk\par
!pip install nltk\par
nltk.download('punkt')\par
[nltk_data] Downloading package punkt to\par
[nltk_data]     C:\\Users\\91842\\AppData\\Roaming\\nltk_data...\par
[nltk_data]   Unzipping tokenizers\\punkt.zip.\par
True\par
df['num_characters'] = df['text'].apply(len)\par
df.head()\par
target\tab text\tab num_characters\par
0\tab 0\tab Go until jurong point, crazy.. Available only ...\tab 111\par
1\tab 0\tab Ok lar... Joking wif u oni...\tab 29\par
2\tab 1\tab Free entry in 2 a wkly comp to win FA Cup fina...\tab 155\par
3\tab 0\tab U dun say so early hor... U c already then say...\tab 49\par
4\tab 0\tab Nah I don't think he goes to usf, he lives aro...\tab 61\par
# num of words\par
df['num_words'] = df['text'].apply(lambda x:len(nltk.word_tokenize(x)))\par
df.head()\par
target\tab text\tab num_characters\tab num_words\par
0\tab 0\tab Go until jurong point, crazy.. Available only ...\tab 111\tab 24\par
1\tab 0\tab Ok lar... Joking wif u oni...\tab 29\tab 8\par
2\tab 1\tab Free entry in 2 a wkly comp to win FA Cup fina...\tab 155\tab 37\par
3\tab 0\tab U dun say so early hor... U c already then say...\tab 49\tab 13\par
4\tab 0\tab Nah I don't think he goes to usf, he lives aro...\tab 61\tab 15\par
df['num_sentences'] = df['text'].apply(lambda x:len(nltk.sent_tokenize(x)))\par
df.head()\par
target\tab text\tab num_characters\tab num_words\tab num_sentences\par
0\tab 0\tab Go until jurong point, crazy.. Available only ...\tab 111\tab 24\tab 2\par
1\tab 0\tab Ok lar... Joking wif u oni...\tab 29\tab 8\tab 2\par
2\tab 1\tab Free entry in 2 a wkly comp to win FA Cup fina...\tab 155\tab 37\tab 2\par
3\tab 0\tab U dun say so early hor... U c already then say...\tab 49\tab 13\tab 1\par
4\tab 0\tab Nah I don't think he goes to usf, he lives aro...\tab 61\tab 15\tab 1\par
df[['num_characters','num_words','num_sentences']].describe()\par
num_characters\tab num_words\tab num_sentences\par
count\tab 5169.000000\tab 5169.000000\tab 5169.000000\par
mean\tab 78.923776\tab 18.456375\tab 1.962275\par
std\tab 58.174846\tab 13.323322\tab 1.433892\par
min\tab 2.000000\tab 1.000000\tab 1.000000\par
25%\tab 36.000000\tab 9.000000\tab 1.000000\par
50%\tab 60.000000\tab 15.000000\tab 1.000000\par
75%\tab 117.000000\tab 26.000000\tab 2.000000\par
max\tab 910.000000\tab 220.000000\tab 38.000000\par
# ham\par
df[df['target'] == 0][['num_characters','num_words','num_sentences']].describe()\par
num_characters\tab num_words\tab num_sentences\par
count\tab 4516.000000\tab 4516.000000\tab 4516.000000\par
mean\tab 70.456820\tab 17.123339\tab 1.815545\par
std\tab 56.356802\tab 13.491315\tab 1.364098\par
min\tab 2.000000\tab 1.000000\tab 1.000000\par
25%\tab 34.000000\tab 8.000000\tab 1.000000\par
50%\tab 52.000000\tab 13.000000\tab 1.000000\par
75%\tab 90.000000\tab 22.000000\tab 2.000000\par
max\tab 910.000000\tab 220.000000\tab 38.000000\par
#spam\par
df[df['target'] == 1][['num_characters','num_words','num_sentences']].describe()\par
num_characters\tab num_words\tab num_sentences\par
count\tab 653.000000\tab 653.000000\tab 653.000000\par
mean\tab 137.479326\tab 27.675345\tab 2.977029\par
std\tab 30.014336\tab 7.011513\tab 1.493676\par
min\tab 13.000000\tab 2.000000\tab 1.000000\par
25%\tab 131.000000\tab 25.000000\tab 2.000000\par
50%\tab 148.000000\tab 29.000000\tab 3.000000\par
75%\tab 157.000000\tab 32.000000\tab 4.000000\par
max\tab 223.000000\tab 46.000000\tab 9.000000\par
import seaborn as sns\par
plt.figure(figsize=(12,6))\par
sns.histplot(df[df['target'] == 0]['num_characters'])\par
sns.histplot(df[df['target'] == 1]['num_characters'],color='red')\par
<AxesSubplot:xlabel='num_characters', ylabel='Count'>\par
\par
plt.figure(figsize=(12,6))\par
sns.histplot(df[df['target'] == 0]['num_words'])\par
sns.histplot(df[df['target'] == 1]['num_words'],color='red')\par
<AxesSubplot:xlabel='num_words', ylabel='Count'>\par
\par
sns.pairplot(df,hue='target')\par
<seaborn.axisgrid.PairGrid at 0x16f88c4a4f0>\par
\par
sns.heatmap(df.corr(),annot=True)\par
<AxesSubplot:>\par
\par
3. Data Preprocessing\par
Lower case\par
Tokenization\par
Removing special characters\par
Removing stop words and punctuation\par
Stemming\par
def transform_text(text):\par
    text = text.lower()\par
    text = nltk.word_tokenize(text)\par
    \par
    y = []\par
    for i in text:\par
        if i.isalnum():\par
            y.append(i)\par
    \par
    text = y[:]\par
    y.clear()\par
    \par
    for i in text:\par
        if i not in stopwords.words('english') and i not in string.punctuation:\par
            y.append(i)\par
            \par
    text = y[:]\par
    y.clear()\par
    \par
    for i in text:\par
        y.append(ps.stem(i))\par
    \par
            \par
    return " ".join(y)\par
transform_text("I'm gonna be home soon and i don't want to talk about this stuff anymore tonight, k? I've cried enough today.")\par
'gon na home soon want talk stuff anymor tonight k cri enough today'\par
df['text'][10]\par
"I'm gonna be home soon and i don't want to talk about this stuff anymore tonight, k? I've cried enough today."\par
from nltk.stem.porter import PorterStemmer\par
ps = PorterStemmer()\par
ps.stem('loving')\par
'love'\par
df['transformed_text'] = df['text'].apply(transform_text)\par
df.head()\par
target\tab text\tab num_characters\tab num_words\tab num_sentences\tab transformed_text\par
0\tab 0\tab Go until jurong point, crazy.. Available only ...\tab 111\tab 24\tab 2\tab go jurong point crazi avail bugi n great world...\par
1\tab 0\tab Ok lar... Joking wif u oni...\tab 29\tab 8\tab 2\tab ok lar joke wif u oni\par
2\tab 1\tab Free entry in 2 a wkly comp to win FA Cup fina...\tab 155\tab 37\tab 2\tab free entri 2 wkli comp win fa cup final tkt 21...\par
3\tab 0\tab U dun say so early hor... U c already then say...\tab 49\tab 13\tab 1\tab u dun say earli hor u c alreadi say\par
4\tab 0\tab Nah I don't think he goes to usf, he lives aro...\tab 61\tab 15\tab 1\tab nah think goe usf live around though\par
from wordcloud import WordCloud\par
wc = WordCloud(width=500,height=500,min_font_size=10,background_color='white')\par
spam_wc = wc.generate(df[df['target'] == 1]['transformed_text'].str.cat(sep=" "))\par
plt.figure(figsize=(15,6))\par
plt.imshow(spam_wc)\par
<matplotlib.image.AxesImage at 0x16f87ea8cd0>\par
\par
ham_wc = wc.generate(df[df['target'] == 0]['transformed_text'].str.cat(sep=" "))\par
plt.figure(figsize=(15,6))\par
plt.imshow(ham_wc)\par
<matplotlib.image.AxesImage at 0x16f87f6c280>\par
\par
df.head()\par
target\tab text\tab num_characters\tab num_words\tab num_sentences\tab transformed_text\par
0\tab 0\tab Go until jurong point, crazy.. Available only ...\tab 111\tab 24\tab 2\tab go jurong point crazi avail bugi n great world...\par
1\tab 0\tab Ok lar... Joking wif u oni...\tab 29\tab 8\tab 2\tab ok lar joke wif u oni\par
2\tab 1\tab Free entry in 2 a wkly comp to win FA Cup fina...\tab 155\tab 37\tab 2\tab free entri 2 wkli comp win fa cup final tkt 21...\par
3\tab 0\tab U dun say so early hor... U c already then say...\tab 49\tab 13\tab 1\tab u dun say earli hor u c alreadi say\par
4\tab 0\tab Nah I don't think he goes to usf, he lives aro...\tab 61\tab 15\tab 1\tab nah think goe usf live around though\par
spam_corpus = []\par
for msg in df[df['target'] == 1]['transformed_text'].tolist():\par
    for word in msg.split():\par
        spam_corpus.append(word)\par
        \par
len(spam_corpus)\par
9941\par
from collections import Counter\par
sns.barplot(pd.DataFrame(Counter(spam_corpus).most_common(30))[0],pd.DataFrame(Counter(spam_corpus).most_common(30))[1])\par
plt.xticks(rotation='vertical')\par
plt.show()\par
C:\\Users\\91842\\anaconda3\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\par
  warnings.warn(\par
\par
ham_corpus = []\par
for msg in df[df['target'] == 0]['transformed_text'].tolist():\par
    for word in msg.split():\par
        ham_corpus.append(word)\par
len(ham_corpus)\par
35303\par
from collections import Counter\par
sns.barplot(pd.DataFrame(Counter(ham_corpus).most_common(30))[0],pd.DataFrame(Counter(ham_corpus).most_common(30))[1])\par
plt.xticks(rotation='vertical')\par
plt.show()\par
C:\\Users\\91842\\anaconda3\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\par
  warnings.warn(\fs48\par
}
 